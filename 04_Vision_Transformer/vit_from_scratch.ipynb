{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Motivation\n",
        "Traditional computer vision models like CNNs process images **locally** where each convolution layer sees only a small region of the image, as they rely on local receptive fields and strong inductive biases (spatial locality and translation equivariance).\n",
        "Although being effective on small and medium-sized datasets, CNNs require deep hierarchies to capture global context.\n",
        "\n",
        "###Why ViT?\n",
        "Vision Transformers (ViT) take a different approach:\n",
        "they they remove convolutional operations entirely and model images as sequences of visual tokens, enabling direct global reasoning via self-attention.\n"
      ],
      "metadata": {
        "id": "_m1ENrOMLGCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import the dependencies"
      ],
      "metadata": {
        "id": "yZxVPnubEyQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "-W1B67vWE60g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load CIFAR-10 dataset"
      ],
      "metadata": {
        "id": "ZnqPvQaYE8x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "blLUG5mXIr54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb42c5c-0458-496a-dc3c-5812ced329dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 31.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Patch Embedding\n",
        "An input image of shape `(H, W, C)` is partitioned into non-overlapping patches of size\n",
        "`P × P`. Each patch is flattened and linearly projected into a vector of dimension `D`.\n"
      ],
      "metadata": {
        "id": "2Nv-1Wf0vb1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels, embed_dim,\n",
        "            kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)            # (B, E, H', W')\n",
        "        x = x.flatten(2)            # (B, E, N)\n",
        "        x = x.transpose(1, 2)       # (B, N, E)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "DH6jrZQgFjg5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Block\n",
        "Transformers do not inherently understand spatial order. Positional embeddings are added to preserve information about **where each patch is located** in the image.\n"
      ],
      "metadata": {
        "id": "HaYeb5DGrmVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "ELdfw9z_GZ55"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vision Transformer Model\n",
        "Self-attention allows each patch to attend to **all other patches** in the image. This enables the model to capture **global relationships** early, rather than relying on stacked convolutions."
      ],
      "metadata": {
        "id": "les8bqGPKbdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, num_classes=10,\n",
        "                 embed_dim=256, depth=6, num_heads=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, num_heads)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        cls_output = x[:, 0]\n",
        "        return self.head(cls_output)"
      ],
      "metadata": {
        "id": "jG0C4tEhKfh4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "giEXdJzzzrKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VisionTransformer().to(\"cuda\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "vGtMGHWI3Pyd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK89jXxWGlRM",
        "outputId": "b771cfd8-41ba-4eb6-e357-cc0f6011abc8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.6314\n",
            "Epoch 2, Loss: 1.2619\n",
            "Epoch 3, Loss: 1.1252\n",
            "Epoch 4, Loss: 1.0384\n",
            "Epoch 5, Loss: 0.9710\n",
            "Epoch 6, Loss: 0.9030\n",
            "Epoch 7, Loss: 0.8483\n",
            "Epoch 8, Loss: 0.7844\n",
            "Epoch 9, Loss: 0.7255\n",
            "Epoch 10, Loss: 0.6616\n",
            "Epoch 11, Loss: 0.5961\n",
            "Epoch 12, Loss: 0.5351\n",
            "Epoch 13, Loss: 0.4638\n",
            "Epoch 14, Loss: 0.3972\n",
            "Epoch 15, Loss: 0.3345\n",
            "Epoch 16, Loss: 0.2838\n",
            "Epoch 17, Loss: 0.2528\n",
            "Epoch 18, Loss: 0.2088\n",
            "Epoch 19, Loss: 0.1896\n",
            "Epoch 20, Loss: 0.1769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "WXvTaJ_VsjUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct, total = 0, 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWwbZ4ujsewL",
        "outputId": "ac765de8-7576-4882-ea2e-fc33638c28a8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 63.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obviously the accuracy is poor. Why?\n",
        "\n",
        "1- ViTs are data-hungry and our dataset is not rich enough:\n",
        "CIFAR-10 is 50k small images (32×32) only\n",
        "\n",
        "2- ViT has weak inductive bias (no locality like CNNs):\n",
        "From scratch → struggles to generalize\n",
        "\n",
        "3- No strong data augmentation:\n",
        "CNNs generalize well even with weak augmentation\n",
        "ViTs depend heavily on: RandomCrop, HorizontalFlip, Color jitter, MixUp / CutMix\n",
        "\n",
        "4- Training duration may be too short as ViTs converge slower than CNNs\n",
        "\n",
        "5- Learning-rate scheduling may help: ViTs are very sensitive to LR Constant LR → suboptimal minima\n",
        "In the next notebook, we will apply these refinements and see whether it works"
      ],
      "metadata": {
        "id": "Z6v69G2otIcO"
      }
    }
  ]
}