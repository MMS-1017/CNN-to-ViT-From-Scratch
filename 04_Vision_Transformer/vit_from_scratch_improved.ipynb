{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Motivation\n",
        "In the previous notebook we implemented ViT from scratch but got poor accuarcy and stated some possible fixes such as:\n",
        "- Stronger augmentation\n",
        "- Longer training\n",
        "- Weight decay\n",
        "- LR scheduler\n",
        "- Proper train/eval modes\n",
        "\n",
        "In this notebook, we will apply these modifications and see whether they will imporve the accuracy or not\n",
        "\n"
      ],
      "metadata": {
        "id": "_m1ENrOMLGCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import the dependencies"
      ],
      "metadata": {
        "id": "yZxVPnubEyQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-W1B67vWE60g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load CIFAR-10 dataset\n",
        "with data augmentation added (RandomCrop & Flip)"
      ],
      "metadata": {
        "id": "ZnqPvQaYE8x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # NEW: Stronger augmentation\n",
        "    transforms.RandomHorizontalFlip(),     # NEW: Helps ViT generalize on small data\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=train_transform\n",
        ")\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=test_transform\n",
        ")\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2\n",
        ")\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=128, shuffle=False, num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "blLUG5mXIr54"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Patch Embedding\n",
        "An input image of shape `(H, W, C)` is partitioned into non-overlapping patches of size\n",
        "`P × P`. Each patch is flattened and linearly projected into a vector of dimension `D`.\n"
      ],
      "metadata": {
        "id": "2Nv-1Wf0vb1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels, embed_dim,\n",
        "            kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "    # Conv2d implementation of patch embedding (efficient)\n",
        "    # Explicit patch_size = 4 for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)        # (B, E, H', W')\n",
        "        x = x.flatten(2)        # (B, E, N)\n",
        "        x = x.transpose(1, 2)   # (B, N, E)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "DH6jrZQgFjg5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Block\n",
        "Transformers do not inherently understand spatial order. Positional embeddings are added to preserve information about **where each patch is located** in the image.\n"
      ],
      "metadata": {
        "id": "HaYeb5DGrmVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim, num_heads, batch_first=True\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(\n",
        "            self.norm1(x), self.norm1(x), self.norm1(x)\n",
        "        )[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "    # Residual connections + layer norm: standard improvement over naive MLP\n"
      ],
      "metadata": {
        "id": "ELdfw9z_GZ55"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vision Transformer Model\n",
        "Self-attention allows each patch to attend to **all other patches** in the image. This enables the model to capture **global relationships** early, rather than relying on stacked convolutions."
      ],
      "metadata": {
        "id": "les8bqGPKbdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        num_classes=10,\n",
        "        embed_dim=256,\n",
        "        depth=6,\n",
        "        num_heads=8\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            img_size, patch_size, 3, embed_dim\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, num_patches + 1, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, num_heads)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return self.head(x[:, 0])"
      ],
      "metadata": {
        "id": "jG0C4tEhKfh4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Optimizer, Scheduler, and Trainer"
      ],
      "metadata": {
        "id": "giEXdJzzzrKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(), lr=3e-4, weight_decay=1e-4\n",
        ")\n",
        "\n",
        "epochs = 50\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=epochs\n",
        ")\n",
        "# - Added CosineAnnealingLR (smooth LR decay)\n",
        "# - Increased epochs from 20 → 50"
      ],
      "metadata": {
        "id": "vGtMGHWI3Pyd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_loss = running_loss / len(trainloader)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK89jXxWGlRM",
        "outputId": "8af5833c-1259-4309-b595-021f3426b6fa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] - Loss: 1.7802\n",
            "Epoch [2/50] - Loss: 1.4346\n",
            "Epoch [3/50] - Loss: 1.2951\n",
            "Epoch [4/50] - Loss: 1.2108\n",
            "Epoch [5/50] - Loss: 1.1575\n",
            "Epoch [6/50] - Loss: 1.1012\n",
            "Epoch [7/50] - Loss: 1.0600\n",
            "Epoch [8/50] - Loss: 1.0264\n",
            "Epoch [9/50] - Loss: 0.9949\n",
            "Epoch [10/50] - Loss: 0.9580\n",
            "Epoch [11/50] - Loss: 0.9336\n",
            "Epoch [12/50] - Loss: 0.9117\n",
            "Epoch [13/50] - Loss: 0.8814\n",
            "Epoch [14/50] - Loss: 0.8507\n",
            "Epoch [15/50] - Loss: 0.8247\n",
            "Epoch [16/50] - Loss: 0.8040\n",
            "Epoch [17/50] - Loss: 0.7716\n",
            "Epoch [18/50] - Loss: 0.7520\n",
            "Epoch [19/50] - Loss: 0.7168\n",
            "Epoch [20/50] - Loss: 0.6964\n",
            "Epoch [21/50] - Loss: 0.6705\n",
            "Epoch [22/50] - Loss: 0.6474\n",
            "Epoch [23/50] - Loss: 0.6181\n",
            "Epoch [24/50] - Loss: 0.5978\n",
            "Epoch [25/50] - Loss: 0.5651\n",
            "Epoch [26/50] - Loss: 0.5419\n",
            "Epoch [27/50] - Loss: 0.5146\n",
            "Epoch [28/50] - Loss: 0.4817\n",
            "Epoch [29/50] - Loss: 0.4544\n",
            "Epoch [30/50] - Loss: 0.4278\n",
            "Epoch [31/50] - Loss: 0.4020\n",
            "Epoch [32/50] - Loss: 0.3781\n",
            "Epoch [33/50] - Loss: 0.3510\n",
            "Epoch [34/50] - Loss: 0.3259\n",
            "Epoch [35/50] - Loss: 0.3022\n",
            "Epoch [36/50] - Loss: 0.2806\n",
            "Epoch [37/50] - Loss: 0.2659\n",
            "Epoch [38/50] - Loss: 0.2424\n",
            "Epoch [39/50] - Loss: 0.2266\n",
            "Epoch [40/50] - Loss: 0.2073\n",
            "Epoch [41/50] - Loss: 0.1949\n",
            "Epoch [42/50] - Loss: 0.1854\n",
            "Epoch [43/50] - Loss: 0.1738\n",
            "Epoch [44/50] - Loss: 0.1662\n",
            "Epoch [45/50] - Loss: 0.1581\n",
            "Epoch [46/50] - Loss: 0.1524\n",
            "Epoch [47/50] - Loss: 0.1499\n",
            "Epoch [48/50] - Loss: 0.1440\n",
            "Epoch [49/50] - Loss: 0.1444\n",
            "Epoch [50/50] - Loss: 0.1420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "WXvTaJ_VsjUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct, total = 0, 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWwbZ4ujsewL",
        "outputId": "a729f4b0-d91c-470c-8238-91605f9ad888"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 76.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy was drastically improved from 66.73 to 76.23%% !! but it is still less than classic resnet. Why?\n",
        "\n",
        "Vision Transformers are known to require large-scale datasets or pretraining to outperform convolutional models.\n",
        "CIFAR-10 is intentionally used here for controlled comparison with CNN, VGG, and ResNet architectures.\n",
        "The achieved accuracy reflects the data-hungry nature of ViTs when trained from scratch.\n",
        "\n",
        "**Conclusion**: Vision Transformers underperform CNNs on small datasets like CIFAR-10 due to weak inductive bias."
      ],
      "metadata": {
        "id": "Z6v69G2otIcO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eai8gb0KYkr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}